{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strongest of all TIME!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "import seaborn as sns; sns.set(); sns.set_context(\"talk\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('pokemon_numbers.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    count = 0 \n",
    "    Class = []\n",
    "    Instance = []\n",
    "    Win_Count = []\n",
    "    Lose_Count = []\n",
    "    Draw_Count = []\n",
    "    ELO = []\n",
    "    Kde_Tier = []\n",
    "    Diff_Tier = []\n",
    "    for row in reader: \n",
    "        #print(row['Class'])\n",
    "        Class.append(row['Class'])\n",
    "        Instance.append(row['Instance'])\n",
    "        Win_Count.append(row['Win_Count'])\n",
    "        Lose_Count.append(row['Lose_Count'])\n",
    "        Draw_Count.append(row['Draw_Count'])\n",
    "        ELO.append(row['ELO'])\n",
    "        Kde_Tier.append(row['Kde_Tier'])\n",
    "        Diff_Tier.append(row['diff_Tier'])\n",
    "sig0 = 1.\n",
    "#print()\n",
    "# Now change the number string to a number list\n",
    "Class = list(map(float, Class))\n",
    "ELO = list(map(float, ELO))\n",
    "Instance = list(map(float, Instance))\n",
    "Win_Count = list(map(float, Win_Count))\n",
    "Lose_Count = list(map(float, Lose_Count))\n",
    "Draw_Count = list(map(float, Draw_Count))\n",
    "Kde_Tier = list(map(float, Kde_Tier))\n",
    "Diff_Tier = list(map(float, Diff_Tier))\n",
    "Class = pd.to_numeric(Class)\n",
    "Win_Count = pd.to_numeric(Win_Count)\n",
    "Lose_Count = pd.to_numeric(Lose_Count)\n",
    "ELO = pd.to_numeric(ELO)\n",
    "\n",
    "#Based off previous battling data sets the error for each was about 2. \n",
    "#e = sig0*np.ones_like(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(18,18))\n",
    "axs[0, 0].scatter(Class, ELO, color='goldenrod')\n",
    "axs[0, 0].set(xlabel='Class', ylabel='ELO', title='Class and ELO data set')\n",
    "axs[0, 1].scatter(ELO, Win_Count, color='red')\n",
    "axs[0, 1].set(xlabel='ELO', ylabel='Win_Count', title='ELO and Win Count data set')\n",
    "axs[1, 0].scatter(ELO, Lose_Count)\n",
    "axs[1, 0].set(xlabel='ELO', ylabel='Loss_Count', title='ELO and Loss_Count data set')\n",
    "axs[1, 1].scatter(Class, Win_Count, color ='purple')\n",
    "axs[1, 1].set(xlabel='Class', ylabel='Win_Count', title='Class and Win Count data set')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Look, Together, at Outliners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, Class and ELO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(theta, Class=Class, ELO=ELO, sigma0=sig0):\n",
    "    delta_y = ELO - theta[0] - theta[1] * Class\n",
    "    return delta_y / sigma0\n",
    "\n",
    " \n",
    "def log_posterior_gaussian(theta):\n",
    "    if (all(theta > 0) and all(theta < 100)):\n",
    "        return -0.5 * np.sum(residuals(theta)**2)\n",
    "    else:\n",
    "        return -np.inf  # recall log(0) = -inf  \n",
    "\n",
    "def squared_loss(theta, Class=Class, ELO=ELO, sigma0=sig0):\n",
    "\n",
    "    delta_y = ELO - theta[0] - theta[1] * Class\n",
    "    return np.sum(0.5 * (delta_y / sigma0) ** 2)\n",
    "\n",
    "theta_MLE = optimize.fmin(squared_loss, [0, 0], disp=False)\n",
    "print(f\"MLE: theta0 = {theta_MLE[0]:.1f}, theta1 = {theta_MLE[1]:.2f}\")\n",
    "\n",
    "xfit = np.linspace(195, 275)\n",
    "yfit = theta_MLE[0] + theta_MLE[1] * xfit\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "ax.errorbar(Class, ELO, sig0, fmt='o', color='goldenrod')\n",
    "ax.plot(xfit, yfit, color='black')\n",
    "ax.set_title('Maximum Likelihood fit: Squared Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = np.linspace(-20, 20)\n",
    "\n",
    "def huber_loss(t, c=3):\n",
    "    return ((abs(t) < c) * 0.5 * t ** 2\n",
    "            + (abs(t) >= c) * -c * (0.5 * c - abs(t)))\n",
    "\n",
    "def total_huber_loss(theta, Class=Class, ELO=ELO, sigma0=sig0, c=1):\n",
    "    return huber_loss((ELO - theta[0] - theta[1] * Class) / sigma0, c).sum()\n",
    "\n",
    "# minimize the total Huber loss for c=3\n",
    "theta2 = optimize.fmin(total_huber_loss, [0, 0], disp=False)\n",
    "print(f\"Huber: theta0 = {theta2[0]:.1f}, theta1 = {theta2[1]:.2f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(Class, ELO, sig0, fmt='o', color='goldenrod')\n",
    "ax.plot(xfit, theta_MLE[0] + theta_MLE[1] * xfit, color='gray',ls='--')\n",
    "ax.plot(xfit, theta2[0] + theta2[1] * xfit, color='green')\n",
    "ax.set_title('Maximum Likelihood fit: Huber loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO and Lose Count \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(theta, ELO=ELO, Lose_Count=Lose_Count, sigma0=sig0):\n",
    "    delta_y = Lose_Count - theta[0] - theta[1] * ELO\n",
    "    return delta_y / sigma0\n",
    "\n",
    " \n",
    "def log_posterior_gaussian(theta):\n",
    "    if (all(theta > 0) and all(theta < 100)):\n",
    "        return -0.5 * np.sum(residuals(theta)**2)\n",
    "    else:\n",
    "        return -np.inf  # recall log(0) = -inf  \n",
    "\n",
    "def squared_loss(theta, ELO=ELO, Lose_Count=Lose_Count, sigma0=sig0):\n",
    "    delta_y = Lose_Count - theta[0] - theta[1] * ELO\n",
    "    return np.sum(0.5 * (delta_y / sigma0) ** 2)\n",
    "\n",
    "theta_MLE = optimize.fmin(squared_loss, [0, 0], disp=False)\n",
    "print(f\"MLE: theta0 = {theta_MLE[0]:.1f}, theta1 = {theta_MLE[1]:.2f}\")\n",
    "\n",
    "# Plot the MLE fit versus the data\n",
    "xfit = np.linspace(195, 3000)\n",
    "yfit = theta_MLE[0] + theta_MLE[1] * xfit\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "ax.errorbar(ELO, Lose_Count, sig0, fmt='o', color='blue')\n",
    "ax.plot(xfit, yfit, color='black')\n",
    "ax.set_title('Maximum Likelihood fit: Squared Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-20, 20)\n",
    "\n",
    "def huber_loss(t, c=3):\n",
    "    return ((abs(t) < c) * 0.5 * t ** 2\n",
    "            + (abs(t) >= c) * -c * (0.5 * c - abs(t)))\n",
    "\n",
    "def total_huber_loss(theta, ELO=ELO, Lose_Count=Lose_Count, sigma0=sig0, c=1):\n",
    "    return huber_loss((Lose_Count - theta[0] - theta[1] * ELO) / sigma0, c).sum()\n",
    "\n",
    "theta2 = optimize.fmin(total_huber_loss, [0, 0], disp=False)\n",
    "print(f\"Huber: theta0 = {theta2[0]:.1f}, theta1 = {theta2[1]:.2f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(ELO, Lose_Count, sig0, fmt='o', color='blue')\n",
    "ax.plot(xfit, theta_MLE[0] + theta_MLE[1] * xfit, color='gray',ls='--')\n",
    "ax.plot(xfit, theta2[0] + theta2[1] * xfit, color='orange')\n",
    "ax.set_title('Maximum Likelihood fit: Huber loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO and Win Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(theta, ELO=ELO, Win_Count=Win_Count, sigma0=sig0):\n",
    "    delta_y = Win_Count - theta[0] - theta[1] * ELO\n",
    "    return delta_y / sigma0\n",
    "\n",
    " \n",
    "def log_posterior_gaussian(theta):\n",
    "    if (all(theta > 0) and all(theta < 100)):\n",
    "        return -0.5 * np.sum(residuals(theta)**2)\n",
    "    else:\n",
    "        return -np.inf  # recall log(0) = -inf  \n",
    "\n",
    "def squared_loss(theta, ELO=ELO, Win_Count=Win_Count, sigma0=sig0):\n",
    "    delta_y = Win_Count - theta[0] - theta[1] * ELO\n",
    "    return np.sum(0.5 * (delta_y / sigma0) ** 2)\n",
    "\n",
    "theta_MLE = optimize.fmin(squared_loss, [0, 0], disp=False)\n",
    "print(f\"MLE: theta0 = {theta_MLE[0]:.1f}, theta1 = {theta_MLE[1]:.2f}\")\n",
    "\n",
    "xfit = np.linspace(195, 3000)\n",
    "yfit = theta_MLE[0] + theta_MLE[1] * xfit\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "ax.errorbar(ELO, Win_Count, sig0, fmt='o', color='red')\n",
    "ax.plot(xfit, yfit, color='black')\n",
    "ax.set_title('Maximum Likelihood fit: Squared Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-20, 20)\n",
    "\n",
    "def huber_loss(t, c=3):\n",
    "    return ((abs(t) < c) * 0.5 * t ** 2\n",
    "            + (abs(t) >= c) * -c * (0.5 * c - abs(t)))\n",
    "\n",
    "def total_huber_loss(theta, ELO=ELO, Win_Count=Win_Count, sigma0=sig0, c=1):\n",
    "    return huber_loss((Win_Count - theta[0] - theta[1] * ELO) / sigma0, c).sum()\n",
    "\n",
    "theta2 = optimize.fmin(total_huber_loss, [0, 0], disp=False)\n",
    "print(f\"Huber: theta0 = {theta2[0]:.1f}, theta1 = {theta2[1]:.2f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(ELO, Win_Count, sig0, fmt='o', color='red')\n",
    "ax.plot(xfit, theta_MLE[0] + theta_MLE[1] * xfit, color='gray',ls='--')\n",
    "ax.plot(xfit, theta2[0] + theta2[1] * xfit, color='green')\n",
    "ax.set_title('Maximum Likelihood fit: Huber loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class and Win Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(theta, Class=Class, Win_Count=Win_Count, sigma0=sig0):\n",
    "    delta_y = Win_Count - theta[0] - theta[1] * Class\n",
    "    return delta_y / sigma0\n",
    "\n",
    " \n",
    "def log_posterior_gaussian(theta):\n",
    "    if (all(theta > 0) and all(theta < 100)):\n",
    "        return -0.5 * np.sum(residuals(theta)**2)\n",
    "    else:\n",
    "        return -np.inf  # recall log(0) = -inf  \n",
    "\n",
    "def squared_loss(theta, Class=Class, Win_Count=Win_Count, sigma0=sig0):\n",
    "    delta_y = Win_Count - theta[0] - theta[1] * Class\n",
    "    return np.sum(0.5 * (delta_y / sigma0) ** 2)\n",
    "\n",
    "theta_MLE = optimize.fmin(squared_loss, [0, 0], disp=False)\n",
    "print(f\"MLE: theta0 = {theta_MLE[0]:.1f}, theta1 = {theta_MLE[1]:.2f}\")\n",
    "\n",
    "xfit = np.linspace(195, 275)\n",
    "yfit = theta_MLE[0] + theta_MLE[1] * xfit\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,6))\n",
    "ax.errorbar(Class, Win_Count, sig0, fmt='o', color='purple')\n",
    "ax.plot(xfit, yfit, color='black')\n",
    "ax.set_title('Maximum Likelihood fit: Squared Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = np.linspace(-20, 20)\n",
    "\n",
    "def huber_loss(t, c=3):\n",
    "    return ((abs(t) < c) * 0.5 * t ** 2\n",
    "            + (abs(t) >= c) * -c * (0.5 * c - abs(t)))\n",
    "\n",
    "def total_huber_loss(theta, Class=Class, Win_Count=Win_Count, sigma0=sig0, c=1):\n",
    "    return huber_loss((Win_Count - theta[0] - theta[1] * Class) / sigma0, c).sum()\n",
    "\n",
    "# minimize the total Huber loss for c=3\n",
    "theta2 = optimize.fmin(total_huber_loss, [0, 0], disp=False)\n",
    "print(f\"Huber: theta0 = {theta2[0]:.1f}, theta1 = {theta2[1]:.2f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(Class, Win_Count, sig0, fmt='o', color='purple')\n",
    "ax.plot(xfit, theta_MLE[0] + theta_MLE[1] * xfit, color='gray',ls='--')\n",
    "ax.plot(xfit, theta2[0] + theta2[1] * xfit, color='yellow')\n",
    "ax.set_title('Maximum Likelihood fit: Huber loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over- and underfittingn and high-degree polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class and ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = max(Class)\n",
    "w = min(Class)\n",
    "x_new=np.linspace(w, q, 100).reshape(100, 1)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,8))\n",
    "Class = Class.reshape(-1, 1)\n",
    "ELO = ELO.reshape(-1, 1)\n",
    "for style, degree in ((\"g-\", 100), (\"b--\", 50), (\"r-.\", 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(Class, ELO)\n",
    "    y_newbig = polynomial_regression.predict(x_new)\n",
    "    ax.plot(x_new, y_newbig, style, label=f'{degree:>3}')\n",
    "    print(f'order {degree:>3}')\n",
    "ax.plot(Class, ELO, \"y.\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"Class\")\n",
    "ax.set_ylim([-500,7500])\n",
    "ax.set_xlim([200,250])\n",
    "ax.set_ylabel(\"ELO\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO and Lose Count \n",
    "Overfitting is occurring at 90 order and 25. So I lowered it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = max(ELO)\n",
    "w = min(ELO)\n",
    "x_new=np.linspace(w, q, 100).reshape(100, 1)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "Lose_Count = Lose_Count.reshape(-1, 1)\n",
    "ELO = ELO.reshape(-1, 1)\n",
    "for style, degree in ((\"g-\", 25), (\"b--\", 15), (\"r-.\", 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(ELO, Lose_Count)\n",
    "    y_newbig = polynomial_regression.predict(x_new)\n",
    "    ax.plot(x_new, y_newbig, style, label=f'{degree:>3}')\n",
    "    print(f'order {degree:>3}')\n",
    "ax.plot(ELO, Lose_Count, \"b.\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"ELO\")\n",
    "ax.set_ylim([-75,700])\n",
    "ax.set_xlim([250,2650])\n",
    "ax.set_ylabel(\"Lose Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO and Win Count \n",
    "ELO was too big for the x values so we shift the analysis to place WIn count as the x value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = max(ELO)\n",
    "w = min(ELO)\n",
    "x_new=np.linspace(w, q, 100).reshape(100, 1)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "Win_Count = Win_Count.reshape(-1, 1)\n",
    "ELO = ELO.reshape(-1, 1)\n",
    "for style, degree in ((\"g-\", 90), (\"b--\", 25), (\"r-.\", 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(ELO, Win_Count)\n",
    "    y_newbig = polynomial_regression.predict(x_new)\n",
    "    ax.plot(x_new, y_newbig, style, label=f'{degree:>3}')\n",
    "    print(f'order {degree:>3}')\n",
    "ax.plot(ELO, Win_Count, \"r.\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel('ELO')\n",
    "ax.set_ylim([-100,600])\n",
    "ax.set_xlim([280,2600])\n",
    "ax.set_ylabel(\"Win Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class and Win Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = max(Class)\n",
    "w = min(Class)\n",
    "x_new=np.linspace(w, q, 100).reshape(100, 1)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,10))\n",
    "Win_Count = Win_Count.reshape(-1, 1)\n",
    "Class = Class.reshape(-1, 1)\n",
    "for style, degree in ((\"g-\", 100), (\"b--\", 75), (\"r-.\", 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(Class, Win_Count)\n",
    "    y_newbig = polynomial_regression.predict(x_new)\n",
    "    ax.plot(x_new, y_newbig, style, label=f'{degree:>3}')\n",
    "    print(f'order {degree:>3}')\n",
    "ax.plot(Class, Win_Count, \"y.\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"$Class$\")\n",
    "ax.set_ylim([-202,1650])\n",
    "ax.set_xlim([200,248])\n",
    "ax.set_ylabel(\"$Win Count$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning of BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(); sns.set_context(\"talk\")\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "# Import theano and pymc3\n",
    "\n",
    "# suppress some theano compilation errors with MacOSX clang compiler\n",
    "import theano\n",
    "# CF added (https://stackoverflow.com/questions/51238578/error-non-constant-expression-cannot-be-narrowed-from-type-npy-intp-to-int)\n",
    "theano.config.gcc.cxxflags = \"-Wno-c++11-narrowing\"  # Is this is really necessary?\n",
    "floatX = theano.config.floatX\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "#print(floatX)\n",
    "\n",
    "Class = scale(Class)\n",
    "Class = Class.reshape(-2, 2)\n",
    "ELO = ELO.reshape(-2,2)\n",
    "#res = Class[0 : 100]\n",
    "#res2 = ELO[0 : 100]\n",
    "\n",
    "print(len(Class))\n",
    "print(len(ELO))\n",
    "Class = Class.astype(floatX)\n",
    "ELO = ELO.astype(floatX)\n",
    "\n",
    "# Split into training and test data. test_size is proportion in test data.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Class, ELO, test_size=.5, \n",
    "                                                    random_state=0)\n",
    "#del Class[58:]\n",
    "#del ELO[58:]\n",
    "#print(res)\n",
    "#print(res2)\n",
    "#print(X_train)\n",
    "#print(X_test)\n",
    "#print(Y_train) \n",
    "#print(Y_test)\n",
    "#print(res[res2==0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_nn(ann_input, ann_output):\n",
    "    n_hidden = 5   # this is the number of neurons, not hidden layers\n",
    "\n",
    "    # Initialize random weights between each layer\n",
    "    init_1 = np.random.randn(Class.shape[1], n_hidden).astype(floatX)\n",
    "    init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "    init_out = np.random.randn(n_hidden).astype(floatX)\n",
    "\n",
    "    with pm.Model() as neural_network:\n",
    "        # Trick: Turn inputs and outputs into shared variables using the data \n",
    "        # container pm.Data.  It's still the same thing, but we can later \n",
    "        # change the values of the shared variable (to switch in the test-data \n",
    "        # later) and pymc3 will just use the new data. Like a pointer we can redirect.\n",
    "        # For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "        ann_input = pm.Data('ann_input', X_train)\n",
    "        ann_output = pm.Data('ann_output', Y_train)\n",
    "\n",
    "        # Weights from input to 1st hidden layer\n",
    "        weights_in_1 = pm.Normal('w_in_1', 0, sigma=1,\n",
    "                                 shape=(Class.shape[1], n_hidden),\n",
    "                                 testval=init_1)\n",
    "\n",
    "        # Weights from 1st to 2nd layer\n",
    "        weights_1_2 = pm.Normal('w_1_2', 0, sigma=1,\n",
    "                                shape=(n_hidden, n_hidden),\n",
    "                                testval=init_2)\n",
    "\n",
    "        # Weights from 2nd hidden layer to output\n",
    "        weights_2_out = pm.Normal('w_2_out', 0, sigma=1,\n",
    "                                  shape=(n_hidden,),\n",
    "                                  testval=init_out)\n",
    "\n",
    "        # Build neural-network using tanh activation function\n",
    "        act_1 = pm.math.tanh( pm.math.dot(ann_input, weights_in_1) )\n",
    "        act_2 = pm.math.tanh( pm.math.dot(act_1, weights_1_2) )\n",
    "        act_out = pm.math.sigmoid( pm.math.dot(act_2, weights_2_out) )\n",
    "\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        out = pm.Bernoulli('out',\n",
    "                           act_out,\n",
    "                           observed=ann_output,\n",
    "                           total_size=Y_train.shape[1] # IMPORTANT for minibatches\n",
    "                          )\n",
    "    return neural_network\n",
    "\n",
    "neural_network = construct_nn(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Class.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
